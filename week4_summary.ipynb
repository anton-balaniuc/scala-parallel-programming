{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallel Programming in Scala \n",
    "\n",
    "## Instructors: Viktor Kincak and Aleksander Prokopek\n",
    "\n",
    "> # Week 4: Data Structures for Parallel Computing\n",
    "\n",
    "**Author:** [Ehsan M. Kermani](https://ca.linkedin.com/in/ehsanmkermani)\n",
    "\n",
    "Codes are available [here](https://github.com/axel22/parprog-snippets/tree/master/src/main/scala/lectures/algorithms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Combiners:\n",
    "\n",
    "* Recall from the previous lecture, `Builder`s can only be used for sequential transformer (i.e. `map, flatMap, filter` etc.) operations. \n",
    "\n",
    "### Builder:\n",
    "\n",
    "```scala\n",
    "trait Builder[A, Repr] { // Repr denotes the type of collection that Builder creates\n",
    "    def +=(elem: A): this.type\n",
    "    def result: Repr\n",
    "}\n",
    "```\n",
    "\n",
    "* To build a *parallel* transformation combiner, we use the `Combiner` abstraction.\n",
    "\n",
    "### Combiner:\n",
    "\n",
    "```scala\n",
    "trait Combiner[A, Repr] extends Builder[A, Repr] {\n",
    "    def combine(that: Combiner[A, Repr]): Combiner[A, Repr] \n",
    "}\n",
    "\n",
    "```\n",
    "\n",
    "**Question:** How to implement an efficient `Combiner`?\n",
    "\n",
    "*Answer:* Let's investing into some possible senarios first.\n",
    "\n",
    "* If `Repr` is a `Set` or `Map`, `combine` represents union\n",
    "* If `Repr` is a sequence, `combine` represents concatenation\n",
    "* `combine` must be of $O(\\log n + \\log m),$ where $n,m$ are the size of inputs\n",
    "* `Array`s cannot be efficiently concatenated (due to occupying  contigious blocks of memory)\n",
    "\n",
    "#### `Set` data structures have efficient lookup, insertion and deletion\n",
    "    \n",
    "    1. Hash tables have expected $O(1),$ lookup, insert and delete\n",
    "    2. Balanced search trees have $O(\\log n),$ lookup, insert and delete\n",
    "    3. Linked lists have $O(n),$ lookup, insert and delete\n",
    "\n",
    "* Most standard `Set` implemetations do not have efficient *union* operation\n",
    "\n",
    "#### `Sequences`\n",
    "\n",
    "    1. Mutable linked lists have $O(1),$ prepend and append complexity while $O(n),$ insertion\n",
    "    2. Functional (cons) linked lists have only $O(1),$ prepend complexity and everything else is of $O(n),$\n",
    "    3. Array lists have *amotized* $O(1),$ append and random access, otherwise $O(n),$\n",
    "\n",
    "* Mutable linked lists have $O(1),$ concatenation while for most other sequences is of $O(n),$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two-Phase Parallel Construction:\n",
    "\n",
    "* Most data structures can be constructed in parallel using the technique called *two-phase construction* through additional intermediate data structure with the following characteristics:\n",
    "\n",
    "    * The intermediate data structure has an efficient `combine` method of $O(\\log n + \\log m),$ or better\n",
    "    * The intermediate data structure has an efficient `+=` method\n",
    "    * Can be converted to the resulting data structure in $O(n/p),$ where $n$ is the size of the data structure and $p$ is the number of processors\n",
    "    \n",
    "The following is an example for concatenating two arrays with *two-phase construction* `ArrayCombiner`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: problems summary ::\n",
      ":::: WARNINGS\n",
      "\tUnable to reparse com.github.alexarchambault.jupyter#jupyter-scala-api_2.10.5;0.2.0-SNAPSHOT from sonatype-snapshots, using Sun Oct 25 12:00:40 PDT 2015\n",
      "\n",
      "\tChoosing sonatype-snapshots for com.github.alexarchambault.jupyter#jupyter-scala-api_2.10.5;0.2.0-SNAPSHOT\n",
      "\n",
      "\tUnable to reparse com.github.alexarchambault#ammonite-api_2.10.5;0.3.1-SNAPSHOT from sonatype-snapshots, using Wed Oct 21 02:44:30 PDT 2015\n",
      "\n",
      "\tChoosing sonatype-snapshots for com.github.alexarchambault#ammonite-api_2.10.5;0.3.1-SNAPSHOT\n",
      "\n",
      "\tUnable to reparse com.github.alexarchambault.jupyter#jupyter-api_2.10;0.2.0-SNAPSHOT from sonatype-snapshots, using Wed Oct 21 08:05:05 PDT 2015\n",
      "\n",
      "\tChoosing sonatype-snapshots for com.github.alexarchambault.jupyter#jupyter-api_2.10;0.2.0-SNAPSHOT\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "load.ivy(\"com.storm-enroute\" %% \"scalameter-core\" % \"0.6\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[36mjava.util.concurrent._\u001b[0m\n",
       "\u001b[32mimport \u001b[36mscala.util.DynamicVariable\u001b[0m\n",
       "defined \u001b[32mobject \u001b[36mcommon\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "/**\n",
    "  Taken from [[https://github.com/axel22/parprog-snippets]]\n",
    "*/\n",
    "\n",
    "import java.util.concurrent._\n",
    "import scala.util.DynamicVariable\n",
    "\n",
    "object common {\n",
    "\n",
    "  val forkJoinPool = new ForkJoinPool\n",
    "\n",
    "  abstract class TaskScheduler {\n",
    "    def schedule[T](body: => T): ForkJoinTask[T]\n",
    "    def parallel[A, B](taskA: => A, taskB: => B): (A, B) = {\n",
    "      val right = task {\n",
    "        taskB\n",
    "      }\n",
    "      val left = taskA\n",
    "      (left, right.join())\n",
    "    }\n",
    "  }\n",
    "\n",
    "  class DefaultTaskScheduler extends TaskScheduler {\n",
    "    def schedule[T](body: => T): ForkJoinTask[T] = {\n",
    "      val t = new RecursiveTask[T] {\n",
    "        def compute = body\n",
    "      }\n",
    "      Thread.currentThread match {\n",
    "        case wt: ForkJoinWorkerThread =>\n",
    "          t.fork()\n",
    "        case _ =>\n",
    "          forkJoinPool.execute(t)\n",
    "      }\n",
    "      t\n",
    "    }\n",
    "  }\n",
    "\n",
    "  val scheduler =\n",
    "    new DynamicVariable[TaskScheduler](new DefaultTaskScheduler)\n",
    "\n",
    "  def task[T](body: => T): ForkJoinTask[T] = {\n",
    "    scheduler.value.schedule(body)\n",
    "  }\n",
    "\n",
    "  def parallel[A, B](taskA: => A, taskB: => B): (A, B) = {\n",
    "    scheduler.value.parallel(taskA, taskB)\n",
    "  }\n",
    "\n",
    "  def parallel[A, B, C, D](taskA: => A, taskB: => B, taskC: => C, taskD: => D): (A, B, C, D) = {\n",
    "    val ta = task { taskA }\n",
    "    val tb = task { taskB }\n",
    "    val tc = task { taskC }\n",
    "    val td = taskD\n",
    "    (ta.join(), tb.join(), tc.join(), td)\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[36mscala.collection.parallel.Combiner\u001b[0m\n",
       "\u001b[32mimport \u001b[36mscala.collection.mutable.ArrayBuffer\u001b[0m\n",
       "\u001b[32mimport \u001b[36mscala.reflect.ClassTag\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.scalameter._\u001b[0m\n",
       "\u001b[32mimport \u001b[36mcommon._\u001b[0m\n",
       "defined \u001b[32mclass \u001b[36mArrayCombiner\u001b[0m\n",
       "defined \u001b[32mobject \u001b[36mArrayCombiner\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// See [[https://github.com/axel22/parprog-snippets/blob/master/src/main/scala/lectures/algorithms/ArrayCombiner.scala]]\n",
    "\n",
    "import scala.collection.parallel.Combiner\n",
    "import scala.collection.mutable.ArrayBuffer\n",
    "import scala.reflect.ClassTag\n",
    "import org.scalameter._\n",
    "import common._\n",
    "\n",
    "class ArrayCombiner[T <: AnyRef: ClassTag](val parallelism: Int)\n",
    "extends Combiner[T, Array[T]] {\n",
    "  private var numElems = 0\n",
    "  private val buffers = new ArrayBuffer[ArrayBuffer[T]]\n",
    "  buffers += new ArrayBuffer[T]\n",
    "\n",
    "  def +=(elem: T) = { // amotized O(1)\n",
    "    buffers.last += elem\n",
    "    numElems += 1\n",
    "    this\n",
    "  }\n",
    "\n",
    "  def combine[N <: T, That >: Array[T]](that: Combiner[N, That]) = { // takes O(p)\n",
    "    (that: @unchecked) match {\n",
    "      case that: ArrayCombiner[T] =>\n",
    "        buffers ++= that.buffers\n",
    "        numElems += that.numElems\n",
    "        this\n",
    "    }\n",
    "  }\n",
    "\n",
    "  def size = numElems\n",
    "\n",
    "  def clear() = buffers.clear()\n",
    "\n",
    "  private def copyTo(array: Array[T], from: Int, end: Int): Unit = {\n",
    "    var i = from\n",
    "    var j = 0\n",
    "    while (i >= buffers(j).length) {\n",
    "      i -= buffers(j).length\n",
    "      j += 1\n",
    "    }\n",
    "    var k = from\n",
    "    while (k < end) {\n",
    "      array(k) = buffers(j)(i)\n",
    "      i += 1\n",
    "      if (i >= buffers(j).length) {\n",
    "        i = 0\n",
    "        j += 1\n",
    "      }\n",
    "      k += 1\n",
    "    }\n",
    "  }\n",
    "\n",
    "  def result: Array[T] = {\n",
    "    val step = math.max(1, numElems / parallelism)\n",
    "    val array = new Array[T](numElems)\n",
    "    val starts = (0 until numElems by step) :+ numElems\n",
    "    val chunks = starts.zip(starts.tail)\n",
    "    val tasks = for ((from, end) <- chunks) yield task {\n",
    "      copyTo(array, from, end)\n",
    "    }\n",
    "    tasks.foreach(_.join())\n",
    "    array\n",
    "  }\n",
    "\n",
    "}\n",
    "\n",
    "object ArrayCombiner {\n",
    "\n",
    "  val standardConfig = config(\n",
    "    Key.exec.minWarmupRuns -> 20,\n",
    "    Key.exec.maxWarmupRuns -> 40,\n",
    "    Key.exec.benchRuns -> 60,\n",
    "    Key.verbose -> true\n",
    "  ) withWarmer(new Warmer.Default)\n",
    "\n",
    "  def main(args: Array[String]) {\n",
    "    val size = 1000000\n",
    "\n",
    "    def run(p: Int) {\n",
    "      val taskSupport = new collection.parallel.ForkJoinTaskSupport(\n",
    "        new scala.concurrent.forkjoin.ForkJoinPool(p))\n",
    "      val strings = (0 until size).map(_.toString)\n",
    "      val time = standardConfig measure {\n",
    "        val parallelized = strings.par\n",
    "        parallelized.tasksupport = taskSupport\n",
    "        def newCombiner = new ArrayCombiner(p): Combiner[String, Array[String]]\n",
    "        parallelized.aggregate(newCombiner)(_ += _, _ combine _).result\n",
    "      }\n",
    "      println(s\"p = $p, time = $time ms\")\n",
    "    }\n",
    "\n",
    "    run(1)\n",
    "    run(2) // almost linear speedup\n",
    "    run(4) // memory bottleneck\n",
    "    run(8) // may even see loss in performance\n",
    "  }\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "// java memory error!\n",
    "// ArrayCombiner.main(Array(\"start\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More on two-phase construction:\n",
    "\n",
    "#### `ArrayCombiner`:\n",
    "\n",
    "1. Partition the indices into subintervals\n",
    "2. Initialize the array in parallel\n",
    "\n",
    "#### `Hash Tables`:\n",
    "\n",
    "1. Partition the hash codes into buckets according to hash prefix\n",
    "2. Allocate the table and map hash codes from different buckets into different regions\n",
    "\n",
    "#### `Search Trees`:\n",
    "\n",
    "1. Partition the elements into non-overlapping intervals according to their ordering\n",
    "2. Costruct search trees in parallel and link non-overlapping trees\n",
    "\n",
    "## Answer to efficient implementation:\n",
    "\n",
    "* *Two-phase construction* (as see above)\n",
    "* When the data structure itself allows efficient concatenation (see next part)\n",
    "* *Concurrent data structures* where different combiners share the same underlying data structure and rely on **synchronization** to correctly update the data structure when `+=` is called"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conc - Tree:\n",
    "\n",
    "* **Conc data type** is the parallel counterpart of the functional cons `List`\n",
    "\n",
    "* `List`s are built for sequential computations while trees allow parallel computations when they can be sufficiently balanced\n",
    "\n",
    "* The following simple Tree abstraction, is *not* suitable for parallel computation as the balance between left and right subtrees can be violated, thus leaving us with sequential structure!\n",
    "\n",
    "```scala\n",
    "sealed trait Tree[+T]\n",
    "case class Node[T](left: Tree[T], right: Tree[T]) extends Tree[T]\n",
    "case class Leaf[T](elem: T) extends Tree[T]\n",
    "case object Empty extends Tree[Nothing]\n",
    "```\n",
    "\n",
    "* `Conc` data type (aka conc-tree) is the remedy for making tree balanced\n",
    "\n",
    "```scala\n",
    "\n",
    "sealed trait Conc[+T] {\n",
    "    val level: Int // longest path from root to leaves\n",
    "    val size: Int // number of elements in the (sub)tree\n",
    "    left: Conc[T]\n",
    "    right: Conc[T]\n",
    "}\n",
    "\n",
    "```\n",
    "\n",
    "One concrete implementation is as follows\n",
    "\n",
    "```scala\n",
    "\n",
    "case object Empty extends Conc[Nothing] {\n",
    "    def level = 0\n",
    "    def size = 0\n",
    "}\n",
    "\n",
    "class Single[T](elem: T) extends Conc[T] {\n",
    "    def level = 0\n",
    "    def size = 1\n",
    "}\n",
    "\n",
    "case class <>[T](left: Conc[T], right: Conc[T]) extends Conc[T] {\n",
    "    val level = 1 + math.max(left.level, right.level)\n",
    "    val size = left.size + right.size\n",
    "}\n",
    "\n",
    "```\n",
    "\n",
    "### Invariants:\n",
    "\n",
    "1. A `<>` node can never contain `Empty` as its subtree\n",
    "2. The leve difference between `left` and `right` subtree of a `<>` node is always $1$ or less (guarantees balance)\n",
    "\n",
    "Resemebles, [AVL-tree](https://en.wikipedia.org/wiki/AVL_tree), but *not necessary* a binary search tree\n",
    "\n",
    "See [this](https://github.com/axel22/parprog-snippets/blob/master/src/main/scala/lectures/algorithms/Conc.scala) for more details.\n",
    "\n",
    "The complexity of concatenation is of $O(\\mid h_1 - h_2\\mid),$ where $h_1, h_2$ are heights of left and right subtrees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $O(1),$-amortized append operation:\n",
    "\n",
    "* This is crucial for implementing `Combiner` efficiently\n",
    "\n",
    "We can implement the `+=` operation in `Combiner` as follows with $O(\\log n),$ complexity\n",
    "\n",
    "```scala\n",
    "val xs: Conc[T] = Empty\n",
    "def +=(elem: T) {\n",
    "    xs = xs <> Single(elem)\n",
    "}\n",
    "```\n",
    "\n",
    "However, by relaxing the above `Conc` data type and add new `Append` node, we can achieve $O(1),$ appends with low constant factor.\n",
    "\n",
    "```scala\n",
    "// relaxes invariant 2 by allowing appending any two Conc trees\n",
    "case class <>[T](left: Conc[T], right: Conc[T]) extends Conc[T] {  \n",
    "    val level = 1 + math.max(left.level, right.level)\n",
    "    val size = left.size + right.size\n",
    "}\n",
    "```\n",
    "\n",
    "Now, we can allocate *constant* (here $2$) objects, when appending a single element to a tree and $O(1),$ operations.\n",
    "\n",
    "```scala\n",
    "def appendLeaf[T](xs: Conc[T], y: T): Conc[T] = new Append(xs, new Single(y)) \n",
    "```\n",
    "\n",
    "But, we need to somehow tranfrom back and eliminate `Append` nodes in $O(\\log n),$ to achieve $O(1),$ complexity. To be able to do that, we impose another condition that for a tree with $n$ elements, we **don't** allow *more than* $O(\\log n),$ `Append` nodes. Otherwise, we will be stuck removing `Append` nodes taking more time than necessary!\n",
    "\n",
    "We can see that adding $n$ leaves to an `Append` list, requires $O(n),$ work, thus the amortized of $O(1),$ for adding a leaf. And storing $n$ leaves requires $O(\\log n),$ `Append` nodes.\n",
    "\n",
    "Now we can reimplement `appendLeaf` as follows\n",
    "\n",
    "```scala\n",
    "def appendLeaf[T](xs: Conc[T], ys: Single[T]): Conc[T] = xs match {\n",
    "    case Empty => y2\n",
    "    case xs: Single[T] => new <>(xs, ys)\n",
    "    case _ <> _ => new Append(xs, ys)\n",
    "    case xs: Append[T] => append(xs, ys)\n",
    "}\n",
    "```\n",
    "See `append` implementation and more via *tail recursion* [here](https://github.com/axel22/parprog-snippets/blob/master/src/main/scala/lectures/algorithms/Conc.scala) for more details.\n",
    "\n",
    "Finally, we have implemented a *immutable* data structure with, $O(1),$-**amortized** *append* and $O(\\log n),$ concatenation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conc - Tree Combiners:\n",
    "\n",
    "The following is about the *mutable* implementation of the previous `Conc` tree.\n",
    "\n",
    "### ConcBuffer:\n",
    "\n",
    "* `ConcBuffer` appends elements into an array of size $k$\n",
    "* When the array gets full, it's stored into a `Chunk` node and added to `Conc-tree`\n",
    "\n",
    "```scala\n",
    "class ConcBuffer[T: ClassTag](val k: Int, private var conc: Conc[T]) {\n",
    "    private var chunk: Array[T] = new Array(k)\n",
    "    private var chunkSize: Int = 0\n",
    "}\n",
    "```\n",
    "If the `chunk` is full, we `expand` and push the array into the `Conc-tree`\n",
    "\n",
    "```scala\n",
    "final def +=(elem: T): Unit = {\n",
    "    if (chunkSize > k) expand()\n",
    "    chunk(chunkSize) = elem\n",
    "    chunkSize += 1\n",
    "}\n",
    "```\n",
    "\n",
    "### Chunk node:\n",
    "\n",
    "* `Chunk` node is similar to `Single` node, but instead of a single element, they hold an array of elements\n",
    "\n",
    "```scala\n",
    "class Chunk[T](val array: Array[T], val size: Int) extends Conc[T] {\n",
    "    def level = 0\n",
    "}\n",
    "```\n",
    "\n",
    "Now with `Chunk` node, `expand` is simple\n",
    "\n",
    "```scala\n",
    "private def expand() {\n",
    "    conc = appendLeaf(conc, new Chunk(chunk, chunkSize))\n",
    "    chunk = new Array(k)\n",
    "    chunkSize = 0\n",
    "}\n",
    "```\n",
    "\n",
    "Hence,\n",
    "\n",
    "```scala\n",
    "final def combine(that: ConcBuffer[T]): ConcBuffer[T] = {\n",
    "    val combinedConc = this.result <> that.result\n",
    "    new ConcBuffer(l, combinedConc)\n",
    "}\n",
    "\n",
    "// result packs `chunk` array into the tree and returns the resulting tree\n",
    "def result: Conc[T] = {\n",
    "    conc = appendLeaf(conc, new Conc.Chunk(chunk, chunkSize))\n",
    "    conc\n",
    "}\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala 2.10",
   "language": "scala210",
   "name": "scala210"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": "scala",
   "mimetype": "text/x-scala",
   "name": "scala210",
   "pygments_lexer": "scala",
   "version": "2.10.5"
  },
  "widgets": {
   "state": {},
   "version": "1.1.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
